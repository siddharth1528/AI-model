{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed76794c-a3d2-42af-9b75-97abf2250241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Connected to MSSQL using SQL Authentication\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "\n",
    "conn_str = (\n",
    "    r\"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    r\"SERVER=MSC-RANGE-KOLK;\"\n",
    "    r\"DATABASE={Wrench Enterprise};\"\n",
    "    r\"UID=wrench;\"\n",
    "    r\"PWD=wrench@123;\"\n",
    ")\n",
    "\n",
    "conn = pyodbc.connect(conn_str)\n",
    "print(\"‚úÖ Connected to MSSQL using SQL Authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "566ba74f-ed1b-4c09-b438-03ea488d2a97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_7228\\2702159710.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_NCR = pd.read_sql(query_1, conn)\n",
      "C:\\Temp\\ipykernel_7228\\2702159710.py:5: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df_FCD = pd.read_sql(query_2, conn)\n"
     ]
    }
   ],
   "source": [
    "query_1 = \"SELECT * FROM [dbo].[NCR_Report_updated]\"  \n",
    "df_NCR = pd.read_sql(query_1, conn)\n",
    "\n",
    "query_2 = \"SELECT * FROM [dbo].[FCD_Report_Internship_Siddhartha]\"  \n",
    "df_FCD = pd.read_sql(query_2, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe4f422-b825-4666-b76b-55beceaa4928",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NCR['DOC_STATUS'] = df_NCR['DOC_STATUS'].replace({\n",
    "    '0': 'Work In Progress',\n",
    "    '3': 'Approved',\n",
    "    '4': None  \n",
    "})\n",
    "df_NCR = df_NCR[df_NCR['DOC_STATUS'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bae71399-db10-4c63-b66e-38d10eb0adbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FCD['DOC_Status'] = df_FCD['DOC_Status'].replace({\n",
    "    0: 'Work In Progress',\n",
    "    3: 'Approved',\n",
    "    4: None  \n",
    "})\n",
    "df_FCD = df_FCD[df_FCD['DOC_Status'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85c6b32d-786d-4e62-9a3f-2b6181caf552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_7228\\272404152.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_FCD.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_FCD.fillna(\"\", inplace=True)\n",
    "df_FCD.columns = df_FCD.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12472cb3-e498-45f3-ae80-852200ed57ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df_FCD['Ongoing_Delay_Days'] = (pd.Timestamp.today() - df_FCD['Current_Stage_Activation_Date']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2fe0db4-e74a-4f7a-a236-7d0c719d0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Temp\\ipykernel_7228\\3638386972.py:1: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  df_NCR.fillna(\"\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "df_NCR.fillna(\"\", inplace=True)\n",
    "df_NCR.columns = df_NCR.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d372d8b-5df8-475b-8c02-23a601197e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import faiss\n",
    "import os\n",
    "import pickle\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings \n",
    "from langchain.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b061ab1-6766-4abb-a5a5-ea01caf159ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.schema.messages import BaseMessage\n",
    "from langchain.schema import ChatResult, ChatGeneration\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel\n",
    "import requests\n",
    "\n",
    "class NvidiaChatLLM(BaseChatModel, BaseModel):\n",
    "    model_name: str\n",
    "    api_key: str\n",
    "    endpoint: str\n",
    "\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"nvidia-chat-llm\"\n",
    "\n",
    "    def _generate(self, messages: List[BaseMessage], stop: Optional[List[str]] = None) -> ChatResult:\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Accept\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        # Convert LangChain messages to API-compatible format\n",
    "        chat_messages = []\n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                role = \"user\"\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                role = \"assistant\"\n",
    "            elif isinstance(msg, SystemMessage):\n",
    "                role = \"system\"\n",
    "            else:\n",
    "                role = \"user\"\n",
    "            chat_messages.append({\"role\": role, \"content\": msg.content})\n",
    "\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"messages\": chat_messages,\n",
    "            \"max_tokens\": 512,\n",
    "            \"temperature\": 0.2,\n",
    "            \"stream\": False\n",
    "        }\n",
    "\n",
    "        response = requests.post(self.endpoint, headers=headers, json=payload)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            content = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return ChatResult(\n",
    "                generations=[ChatGeneration(message=AIMessage(content=content))]\n",
    "            )\n",
    "        else:\n",
    "            raise RuntimeError(f\"API Error: {response.status_code} - {response.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d40a39-120d-473e-af03-8c2a6460c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the custom NVIDIA LLM\n",
    "llm = NvidiaChatLLM(\n",
    "    model_name=\"meta/llama-4-maverick-17b-128e-instruct\",\n",
    "    api_key=\"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\",\n",
    "    endpoint=\"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fcf5a6d-4063-42da-9389-5e7cde327d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "chat_chain = ConversationChain(llm=llm, memory=memory, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d583b951-77b0-475f-9e8f-8a380273e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    for col in df.select_dtypes(include='object').columns:\n",
    "        df[col] = df[col].astype(str).str.strip().str.upper()\n",
    "\n",
    "    numeric_cols = ['Ongoing_Delay_Days']\n",
    "    for col in numeric_cols:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    datetime_cols = [\n",
    "        'created_on', 'Validate_Stage_Activation', 'Validate_Stage_Completion',\n",
    "        'Approval_Stage_Activation', 'Approval_Stage_Completion',\n",
    "        'Current_Stage_Activation_Date'\n",
    "    ]\n",
    "    for col in datetime_cols:\n",
    "        if col in df.columns and not pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7138b662-1db3-4e9f-8cc4-f2c8271f5c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_NCR= preprocess_df(df_NCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "539771b4-ae52-47b9-8efc-a5d168de3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_FCD= preprocess_df(df_FCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "835bc06f-3b65-4192-a9f2-62e079c40c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert both DataFrames to text rows\n",
    "rows_ncr = df_NCR.to_dict(orient='records')\n",
    "rows_fcd = df_FCD.to_dict(orient='records')\n",
    "\n",
    "# Convert each row into a plain text format\n",
    "doc_texts_ncr = [\"NCR_ENTRY\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in row.items()]) for row in rows_ncr]\n",
    "doc_texts_fcd = [\"FCD_ENTRY\\n\" + \"\\n\".join([f\"{k}: {v}\" for k, v in row.items()]) for row in rows_fcd]\n",
    "\n",
    "# Combine all text entries\n",
    "all_docs = doc_texts_ncr + doc_texts_fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2d0711ad-2351-4c45-9a29-3372de052237",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "docs = splitter.create_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "046fd627-92af-4276-99b0-91a5cb1a6448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed using LLM \n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectorstore = FAISS.from_documents(docs, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "859d31fb-4c4d-4665-9495-4efcda8ed328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save vectorstore for future use\n",
    "with open(\"faiss_index.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorstore, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466812e6-92e6-4950-92a6-44984204370e",
   "metadata": {},
   "source": [
    "NVIDIA_API_KEY = \"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\"\n",
    "NVIDIA_API_URL = \"https://api.nvcf.nvidia.com/v2/nvcf/pexec/functions/meta/llama3-8b-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bd2df1c8-eb26-496b-a64d-f8cb0129ca4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_nvidia_llm(prompt: str):\n",
    "    invoke_url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "    stream = False\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": \"Bearer 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\",  # replace with your key\n",
    "        \"Accept\": \"text/event-stream\" if stream else \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"meta/llama-4-maverick-17b-128e-instruct\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 1.0,\n",
    "        \"frequency_penalty\": 0.0,\n",
    "        \"presence_penalty\": 0.0,\n",
    "        \"stream\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(invoke_url, headers=headers, json=payload)\n",
    "        response.raise_for_status()  # Raises error for 4xx/5xx responses\n",
    "\n",
    "        try:\n",
    "            result = response.json()\n",
    "            return result['choices'][0]['message']['content']\n",
    "        except (ValueError, KeyError) as json_err:\n",
    "            return f\"‚ùå Failed to parse response: {json_err} | Response text: {response.text}\"\n",
    "\n",
    "    except requests.exceptions.RequestException as req_err:\n",
    "        return f\"‚ùå Request error: {req_err}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ffca40b-856b-4514-bcb8-962faf4ef117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_execute_pandas_code(code: str, df_NCR, df_FCD):\n",
    "    import re\n",
    "    import io\n",
    "    import contextlib\n",
    "\n",
    "    matches = re.findall(r\"```(?:python)?\\n(.*?)```\", code, re.DOTALL)\n",
    "    code_to_run = matches[-1].strip() if matches else code.strip()\n",
    "\n",
    "    if \"={\" in code_to_run or \"pd.DataFrame\" in code_to_run:\n",
    "        return \"‚ùå Generated code is trying to create dummy data. Please rephrase your question.\"\n",
    "\n",
    "    if not code_to_run.strip().startswith(\"print\") and \"print(\" not in code_to_run:\n",
    "        code_to_run = f\"print({code_to_run})\"\n",
    "\n",
    "    local_vars = {\n",
    "        'df_NCR': df_NCR,\n",
    "        'df_FCD': df_FCD,\n",
    "        'pd': pd\n",
    "    }\n",
    "\n",
    "    output = io.StringIO()\n",
    "    try:\n",
    "        with contextlib.redirect_stdout(output):\n",
    "            exec(code_to_run, {}, local_vars)\n",
    "        return output.getvalue().strip() or \"‚úÖ Code executed successfully.\"\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Execution error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9a8d10be-3927-442a-a4ec-0ad01f36485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(query: str, k: int = 10):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3bbc6611-2b11-4697-b0a8-022ff01c85f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks_and_answer(query: str, k: int = 5):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    retrieved = retriever.get_relevant_documents(query)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in retrieved])\n",
    "    prompt = f\"Answer the question using the context below:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    return call_nvidia_llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1961e705-c0a6-440f-a9c8-d9324f394701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_query(query: str):\n",
    "    logic_keywords = [\n",
    "        \"how many\", \"list\", \"count\", \"filter\", \"greater than\", \"less than\",\n",
    "        \"equal to\", \"duration\", \"pending\", \"show\", \"match\", \"which\", \"entries\",\n",
    "        \"merge\", \"compare\", \"joined with\", \"approved by\", \"assigned to\"\n",
    "    ]\n",
    "    \n",
    "    summary_keywords = [\n",
    "        \"what is this document\", \"describe this\", \"what does this document talk about\",\n",
    "        \"summarize\", \"overall theme\", \"key issues\", \"highlights\"\n",
    "    ]\n",
    "\n",
    "    query_lower = query.lower()\n",
    "\n",
    "    if any(kw in query_lower for kw in logic_keywords):\n",
    "        return \"logic\"\n",
    "    elif any(kw in query_lower for kw in summary_keywords):\n",
    "        return \"summary\"\n",
    "    else:\n",
    "        return \"chat\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "81778bb2-71ca-44f4-8a25-48b1bbd6d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pandas_prompt(query, df_NCR, df_FCD, chat_history=None):\n",
    "    return f\"\"\"\n",
    "You are a highly skilled Pandas expert and data analyst.\n",
    "\n",
    "You are working with **two Pandas DataFrames**:\n",
    "- `df_NCR`: Contains Non-Conformance Report (NCR) data with the following columns:\n",
    "  {list(df_NCR.columns)}\n",
    "- `df_FCD`: Contains Field Change Document (FCD) data with the following columns:\n",
    "  {list(df_FCD.columns)}\n",
    "\n",
    "You may use either or both DataFrames as needed. These tables can be **joined on the common column `ORDER_NO`** when appropriate.\n",
    "\n",
    "Your task:\n",
    "- Understand the user's question and generate **robust, readable, and executable Pandas code** that directly answers it.\n",
    "- Do NOT create or simulate data using dictionaries or hardcoded values. The DataFrames `df_NCR` and `df_FCD` are already populated with real data.\n",
    "- Ensure the final result is **wrapped in a `print(...)` statement** so that it returns an answer.\n",
    "- Use `.str.upper()` for case-insensitive string comparisons when checking textual fields.\n",
    "- Use `pd.to_numeric(..., errors='coerce')` when comparing numeric fields that may contain nulls or strings.\n",
    "- Handle potential missing values (`NaN`, `None`) safely.\n",
    "- Apply `.dropna()` or conditionals only if necessary.\n",
    "- If querying by date, use `pd.to_datetime(..., errors='coerce')` to convert string columns to datetime format.\n",
    "- Use `.merge(...)` when joining the two tables is needed, and clearly specify the join logic.\n",
    "\n",
    "**Special semantic mapping rules (normalize synonyms):**\n",
    "- Treat the following status-related terms as equivalent when filtering rows:\n",
    "  - `\"WIP\"`, `\"in progress\"`, `\"not started\"`, `\"pending\"`, `\"on hold\"` ‚Üí `\"WORK IN PROGRESS\"`\n",
    "  - `\"done\"`, `\"finished\"`, `\"complete\"`, `\"completed\"` ‚Üí `\"APPROVED\"`\n",
    "- When matching these values in filters, convert both the input and the DataFrame column to uppercase using `.str.upper()`, and compare against the normalized value.\n",
    "\n",
    " **Context awareness**:\n",
    "- If the user‚Äôs query includes vague terms like `\"such\"`, `\"those\"`, or `\"these\"`, interpret them based on the last question or answer shown in the conversation below.\n",
    "- Reuse the same filters or column logic from the previous step if it applies (e.g., delay > 30 days, status = approved, etc.).\n",
    "- Be concise and accurate in continuing the logic without asking for clarification.\n",
    "\n",
    "---\n",
    "Conversation so far:\n",
    "{chat_history or '[no prior context]'}\n",
    "\n",
    "Now generate only Pandas code that answers the following user query:\n",
    "{query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "97be6d6f-cf8e-471b-add8-64cd8592058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_context():\n",
    "    return memory.buffer.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "135dd6b3-04e2-49ba-b598-ef60b8ad1fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_query(query: str):\n",
    "    intent = classify_query(query)\n",
    "\n",
    "    if intent == \"logic\":\n",
    "        # Pull previous memory\n",
    "        chat_history = memory.buffer.strip() if memory.buffer else \"[no prior context]\"\n",
    "\n",
    "        # Build LLM prompt with history\n",
    "        prompt = build_pandas_prompt(query, df_NCR, df_FCD, chat_history=chat_history)\n",
    "\n",
    "        # Generate Pandas code from LLM\n",
    "        code = call_nvidia_llm(prompt)\n",
    "        print(\"\\nGenerated Code:\\n\", code)\n",
    "\n",
    "        # üîß Manual memory injection to keep conversation in buffer\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "        memory.chat_memory.add_ai_message(code)\n",
    "\n",
    "        # Execute and return result\n",
    "        return safe_execute_pandas_code(code, df_NCR=df_NCR, df_FCD=df_FCD)\n",
    "\n",
    "    elif intent == \"summary\":\n",
    "        context = retrieve_context(query)\n",
    "        if not context.strip():\n",
    "            return \"‚ùå No relevant context found in the document. Please try a more specific question.\"\n",
    "\n",
    "        prompt = f\"\"\"\n",
    "        You are a data analyst AI that answers strictly based on structured engineering project records. \n",
    "        Only use the provided document excerpt ‚Äî do not invent or guess if the answer is not explicitly present.\n",
    "\n",
    "        Document Excerpt:\n",
    "        -----------------\n",
    "        {context}\n",
    "        -----------------\n",
    "\n",
    "        Now answer this question based only on the document above:\n",
    "        Q: {query}\n",
    "        \"\"\"\n",
    "        return chat_chain.run(prompt)\n",
    "\n",
    "    else:\n",
    "        return chat_chain.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "66b8f336-7945-466d-8ff2-bbf7a35d11c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid RAG + Logic QA System with Memory Ready ‚úÖ\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question (or type 'exit'):  what column names are in FCD document?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Q: what column names are in FCD document?\n",
      "üß† A: To find the column names in the FCD document dataframe (`df_FCD`), you can use the `columns` attribute. Here's how you can do it:\n",
      "\n",
      "```python\n",
      "# Get the column names of df_FCD\n",
      "fcd_column_names = df_FCD.columns.tolist()\n",
      "\n",
      "# Print the column names\n",
      "print(fcd_column_names)\n",
      "```\n",
      "\n",
      "This will output a list of column names present in the `df_FCD` dataframe.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question (or type 'exit'):  show the column names\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Code:\n",
      " ```python\n",
      "# Get the column names of df_NCR\n",
      "ncr_column_names = df_NCR.columns.tolist()\n",
      "\n",
      "# Get the column names of df_FCD\n",
      "fcd_column_names = df_FCD.columns.tolist()\n",
      "\n",
      "# Print the column names for both DataFrames\n",
      "print(\"df_NCR columns:\", ncr_column_names)\n",
      "print(\"df_FCD columns:\", fcd_column_names)\n",
      "```\n",
      "\n",
      "‚úÖ Q: show the column names\n",
      "üß† A: df_NCR columns: ['DOC_NO', 'DOC_DESCRIPTION', 'ORDER_NO', 'ORDER_DESCRIPTION', 'DOC_STATUS', 'IDOC_ID', 'INT_REV_NO', 'created_on', 'Area', 'Sub Area', 'Discipline', 'Sub contractor', 'Technical Requirements', 'Existing Condition', 'Recommended Disposition', 'Potential Cost Impact', 'Potential Schedule Impact', 'Final Disposition', 'Validate_Stage_Activation', 'Validate_Stage_Completion', 'Validate_Stage_user', 'Approval_Stage_Activation', 'Approval_Stage_Completion', 'Approval_Stage_user', 'Ref_Document_Desc', 'Ref_Document_No', 'Ref_Document_Rev', 'Current_Workflow_stage', 'Workflow_stage_users', 'Current_Stage_Activation_Date', 'Ongoing_Delay_Days', 'DOC_STATUS_NORMALIZED']\n",
      "df_FCD columns: ['DOC_Number', 'DOC_Description', 'Order_No', 'Order_Description', 'DOC_Status', 'IDOC_ID', 'INT_REV_NO', 'Created on', 'Area', 'Sub_Area', 'Discipline', 'Sub contractor', 'Technical Requirments', 'Existing Condition', 'Recommended Disposition', 'Potential Cost Impact', 'Potential Schedule Impact', 'Final Disposition', 'Review_Stage_Activation', 'Review_Stage_Completion', 'Review_Stage_User', 'Validate_Stage_Activation', 'Validate_Stage_Completion', 'Validate_Stage_User', 'Approval_Stage_Activation', 'Approval_Stage_Completion', 'Apporval_Stage_User', 'Ref_Document_Desc', 'Ref_Document_No', 'Ref_Document_Rev', 'Current_Workflow_Stage', 'Workflow_Stage_Users', 'Current_Stage_Activation_Date', 'Ongoing_Delay_Days', 'DOC_Status_Normalized']\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask your question (or type 'exit'):  exit\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Hybrid RAG + Logic QA System with Memory Ready ‚úÖ\")\n",
    "    while True:\n",
    "        user_query = input(\"\\nAsk your question (or type 'exit'): \")\n",
    "        if user_query.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "        response = answer_query(user_query)\n",
    "        print(f\"\\n‚úÖ Q: {user_query}\\nüß† A: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9635601-cfee-493b-a08e-888789d1653a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
